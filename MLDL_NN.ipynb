{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miscrose/ml/blob/main/MLDL_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ka0TvZk5Pha",
        "outputId": "98cdb98a-527b-47a9-bbcf-1fe613711add"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.12/dist-packages (0.13.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikeras) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.5.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (25.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras>=3.2.0->scikeras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSfj8Ht2xPc-",
        "outputId": "5cd0451a-779d-4c79-8120-ee74dbc13eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.0914\n",
            "Loss: 0.0914, Accuracy: 1.0000\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "Prédictions pour de nouvelles données :\n",
            "Caractéristiques : [0 1], Prédiction : 0.9276\n",
            "Caractéristiques : [1 0], Prédiction : 0.9250\n"
          ]
        }
      ],
      "source": [
        "# Importer les bibliothèques nécessaires\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
        "#from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# Supposons que nous ayons des données sur deux caractéristiques et que nous essayons de prédire une variable binaire.\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Caractéristiques\n",
        "y = np.array([0, 1, 1, 1])  # Étiquettes de classe (0 ou 1)\n",
        "\n",
        "# Créer un modèle séquentiel\n",
        "model = Sequential()\n",
        "\n",
        "# Ajouter une couche Dense (perceptron)\n",
        "model.add(Dense(1, input_dim=2, activation='sigmoid'))\n",
        "\n",
        "# Compiler le modèle\n",
        "model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate=0.1), metrics=['accuracy'])\n",
        "\n",
        "# Entraîner le modèle\n",
        "model.fit(X, y, epochs=1000, verbose=0)\n",
        "\n",
        "# Évaluer le modèle\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(f'Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Prédire de nouvelles données\n",
        "new_data = np.array([[0, 1], [1, 0]])\n",
        "predictions = model.predict(new_data)\n",
        "print(\"Prédictions pour de nouvelles données :\")\n",
        "for i in range(len(new_data)):\n",
        "    print(f'Caractéristiques : {new_data[i]}, Prédiction : {predictions[i][0]:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importer les bibliothèques nécessaires\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Données d'exemple (hypothétiques pour la criminalistique)\n",
        "# Supposons que nous ayons des données sur plusieurs caractéristiques liées à des cas criminels et que nous essayons de prédire si un acte criminel a été commis ou non.\n",
        "X = np.random.rand(100, 10)  # 100 échantillons avec 10 caractéristiques hypothétiques\n",
        "y = np.random.randint(2, size=100)  # Étiquettes de classe binaire (0 ou 1)\n",
        "\n",
        "# Créer un modèle séquentiel\n",
        "model = Sequential()\n",
        "\n",
        "# Ajouter une couche Dense (couches cachées)\n",
        "model.add(Dense(32, input_dim=10, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "\n",
        "# Ajouter une couche de sortie\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compiler le modèle\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Entraîner le modèle\n",
        "model.fit(X, y, epochs=50, batch_size=10, verbose=0)\n",
        "\n",
        "# Évaluer le modèle sur les données d'entraînement\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(f'Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Prédire de nouvelles données\n",
        "new_data = np.random.rand(5, 10)  # De nouvelles données hypothétiques\n",
        "predictions = model.predict(new_data)\n",
        "print(\"Prédictions pour de nouvelles données :\")\n",
        "for i in range(len(new_data)):\n",
        "    print(f'Caractéristiques : {new_data[i]}, Prédiction : {predictions[i][0]:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypX6VpR1z5ny",
        "outputId": "335e22f8-7524-431e-e9e4-4b161b5d8f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6814 - loss: 0.6135 \n",
            "Loss: 0.6171, Accuracy: 0.6800\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step\n",
            "Prédictions pour de nouvelles données :\n",
            "Caractéristiques : [0.70154216 0.86290972 0.15191984 0.03308288 0.67552867 0.14069469\n",
            " 0.37286043 0.84949397 0.14302777 0.95759656], Prédiction : 0.4221\n",
            "Caractéristiques : [0.68353155 0.31146547 0.27419095 0.33637767 0.43026197 0.44919278\n",
            " 0.65175337 0.45415704 0.79574971 0.54834388], Prédiction : 0.5191\n",
            "Caractéristiques : [0.02877622 0.21921796 0.68895969 0.30611319 0.07913757 0.36730504\n",
            " 0.47942191 0.0746766  0.24996594 0.40719751], Prédiction : 0.5312\n",
            "Caractéristiques : [0.53937068 0.10431865 0.82874016 0.77257929 0.86271918 0.36807744\n",
            " 0.48238015 0.44460534 0.69530557 0.24154126], Prédiction : 0.5249\n",
            "Caractéristiques : [0.36934259 0.63920132 0.56761166 0.8229538  0.97300029 0.62995876\n",
            " 0.22901416 0.51743642 0.42869175 0.81786706], Prédiction : 0.5258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Générer des données d'exemple (hypothétiques pour la criminalistique)\n",
        "X = np.random.rand(100, 10)\n",
        "y = np.random.randint(2, size=100)\n",
        "\n",
        "# Diviser les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Définir une fonction pour créer et compiler un modèle Keras\n",
        "def create_model(architecture, activation):\n",
        "    model = Sequential()\n",
        "    for num_neurons in architecture:\n",
        "        model.add(Dense(num_neurons, activation=activation))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Créer une liste d'architectures à tester\n",
        "architectures = [\n",
        "    [32, 16],\n",
        "    [64, 32, 16],\n",
        "    [128, 64, 32],\n",
        "]\n",
        "\n",
        "# Créer une liste d'activations à tester\n",
        "activations = ['relu', 'tanh']\n",
        "\n",
        "# Créer un dictionnaire de paramètres pour la recherche sur la grille\n",
        "param_grid = dict(architecture=architectures, activation=activations)\n",
        "\n",
        "best_score = 0\n",
        "best_params = None\n",
        "\n",
        "# Effectuer la recherche sur la grille\n",
        "for architecture in architectures:\n",
        "    for activation in activations:\n",
        "        model = create_model(architecture, activation)\n",
        "        model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
        "        if accuracy > best_score:\n",
        "            best_score = accuracy\n",
        "            best_params = (architecture, activation)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(f\"Meilleur score d'entraînement : {best_score:.4f}\")\n",
        "print(f\"Meilleurs paramètres : {best_params}\")\n",
        "\n",
        "# Évaluer le modèle optimal sur l'ensemble de test\n",
        "best_model = create_model(best_params[0], best_params[1])\n",
        "best_model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, (y_pred > 0.5).astype(int))\n",
        "print(f\"Précision sur l'ensemble de test : {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AlJGAbv6lCz",
        "outputId": "ea0de469-df77-43d4-a35d-eb7db159fdf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b486af97ba0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b48881865c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "Meilleur score d'entraînement : 0.6000\n",
            "Meilleurs paramètres : ([32, 16], 'relu')\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "Précision sur l'ensemble de test : 0.6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# 1. Génération de données simples : y = 3x + 2 + bruit\n",
        "np.random.seed(42)\n",
        "X = np.linspace(-1, 1, 200).reshape(-1, 1).astype(\"float32\")\n",
        "y = (3 * X + 2 + 0.1 * np.random.randn(*X.shape)).astype(\"float32\")\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Input(shape=(1,)),\n",
        "        Dense(8, activation='relu'),\n",
        "        Dense(1)     # sortie scalaire\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_with_batch_size(batch_size, label):\n",
        "    model = create_model()\n",
        "    optimizer = SGD(learning_rate=0.1)\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "    history = model.fit(X, y,\n",
        "                        epochs=50,\n",
        "                        batch_size=batch_size,\n",
        "                        verbose=0,\n",
        "                        shuffle=True)\n",
        "    final_loss = history.history['loss'][-1]\n",
        "    final_mae = history.history['mae'][-1]\n",
        "    print(f\"{label} (batch_size={batch_size}) - Loss: {final_loss:.4f}, MAE: {final_mae:.4f}\")\n",
        "    return model\n",
        "\n",
        "# Stochastic Gradient Descent : batch_size = 1\n",
        "model_sgd = train_with_batch_size(1, \"SGD (stochastique)\")\n",
        "\n",
        "# Batch Gradient Descent : batch_size = N (tout le dataset)\n",
        "model_batch = train_with_batch_size(len(X), \"Batch GD (plein batch)\")\n",
        "\n",
        "# Mini-Batch Gradient Descent : batch_size = 16\n",
        "model_mini = train_with_batch_size(16, \"Mini-batch GD\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCZza6Gr0ggb",
        "outputId": "1a174af8-8799-43cd-f5c4-64aacafd4851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD (stochastique) (batch_size=1) - Loss: 0.0222, MAE: 0.1201\n",
            "Batch GD (plein batch) (batch_size=200) - Loss: 0.0220, MAE: 0.1223\n",
            "Mini-batch GD (batch_size=16) - Loss: 0.0095, MAE: 0.0785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Générer des données d'exemple (classification binaire)\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalisation\n",
        "\n",
        "# Créer un modèle séquentiel\n",
        "model = Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    BatchNormalization(),  # Normalisation de lot\n",
        "    keras.layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)),  # Régularisation L2\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compiler le modèle\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Définir l'arrêt précoce\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Entraîner le modèle avec l'arrêt précoce\n",
        "history = model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Évaluer le modèle\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20GCp1ilFA_o",
        "outputId": "44cdfea4-668e-47be-b585-6a283352f6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.1003 - loss: -59129.5977 - val_accuracy: 0.1000 - val_loss: -827368.9375\n",
            "Epoch 2/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.1001 - loss: -1570881.3750 - val_accuracy: 0.1000 - val_loss: -4857315.5000\n",
            "Epoch 3/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1032 - loss: -6705805.5000 - val_accuracy: 0.1000 - val_loss: -13397955.0000\n",
            "Epoch 4/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.1001 - loss: -16645299.0000 - val_accuracy: 0.1000 - val_loss: -28478000.0000\n",
            "Epoch 5/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1028 - loss: -32816896.0000 - val_accuracy: 0.1000 - val_loss: -51471616.0000\n",
            "Epoch 6/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1027 - loss: -56737208.0000 - val_accuracy: 0.0958 - val_loss: -77861368.0000\n",
            "Epoch 7/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.1016 - loss: -90099360.0000 - val_accuracy: 0.1029 - val_loss: -125132816.0000\n",
            "Epoch 8/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1013 - loss: -133181200.0000 - val_accuracy: 0.1048 - val_loss: -179555968.0000\n",
            "Epoch 9/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1020 - loss: -186695056.0000 - val_accuracy: 0.1092 - val_loss: -223362160.0000\n",
            "Epoch 10/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1003 - loss: -251963792.0000 - val_accuracy: 0.1076 - val_loss: -307993888.0000\n",
            "Epoch 11/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1026 - loss: -331721120.0000 - val_accuracy: 0.1019 - val_loss: -410123680.0000\n",
            "Epoch 12/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1014 - loss: -424657728.0000 - val_accuracy: 0.1023 - val_loss: -485777984.0000\n",
            "Epoch 13/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1033 - loss: -533946080.0000 - val_accuracy: 0.1006 - val_loss: -655851200.0000\n",
            "Epoch 14/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1036 - loss: -660583488.0000 - val_accuracy: 0.1039 - val_loss: -792101952.0000\n",
            "Epoch 15/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1019 - loss: -802934208.0000 - val_accuracy: 0.1082 - val_loss: -890437312.0000\n",
            "Epoch 16/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1018 - loss: -964704320.0000 - val_accuracy: 0.1043 - val_loss: -1102873728.0000\n",
            "Epoch 17/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.1030 - loss: -1149690624.0000 - val_accuracy: 0.1006 - val_loss: -1363047040.0000\n",
            "Epoch 18/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1025 - loss: -1341328128.0000 - val_accuracy: 0.1003 - val_loss: -1596912512.0000\n",
            "Epoch 19/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1015 - loss: -1567313920.0000 - val_accuracy: 0.1031 - val_loss: -1842921984.0000\n",
            "Epoch 20/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.1018 - loss: -1815764736.0000 - val_accuracy: 0.1097 - val_loss: -1974462976.0000\n",
            "Epoch 21/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1022 - loss: -2085366784.0000 - val_accuracy: 0.1055 - val_loss: -2229019136.0000\n",
            "Epoch 22/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1031 - loss: -2363728640.0000 - val_accuracy: 0.1011 - val_loss: -2640029952.0000\n",
            "Epoch 23/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.1004 - loss: -2692717824.0000 - val_accuracy: 0.1042 - val_loss: -2953887232.0000\n",
            "Epoch 24/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1025 - loss: -3038251264.0000 - val_accuracy: 0.1033 - val_loss: -3362556160.0000\n",
            "Epoch 25/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1013 - loss: -3428119552.0000 - val_accuracy: 0.1018 - val_loss: -3783648512.0000\n",
            "Epoch 26/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.1001 - loss: -3839642624.0000 - val_accuracy: 0.1024 - val_loss: -4390777856.0000\n",
            "Epoch 27/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1032 - loss: -4262899712.0000 - val_accuracy: 0.1032 - val_loss: -4743581696.0000\n",
            "Epoch 28/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1018 - loss: -4737784320.0000 - val_accuracy: 0.1117 - val_loss: -5009350144.0000\n",
            "Epoch 29/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.1045 - loss: -5226002944.0000 - val_accuracy: 0.1042 - val_loss: -5658356736.0000\n",
            "Epoch 30/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - accuracy: 0.1026 - loss: -5770519040.0000 - val_accuracy: 0.1044 - val_loss: -6310361088.0000\n",
            "Epoch 31/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1034 - loss: -6320287232.0000 - val_accuracy: 0.0959 - val_loss: -6822184960.0000\n",
            "Epoch 32/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.1033 - loss: -6904361984.0000 - val_accuracy: 0.1009 - val_loss: -7975385088.0000\n",
            "Epoch 33/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1034 - loss: -7545215488.0000 - val_accuracy: 0.1048 - val_loss: -8364993024.0000\n",
            "Epoch 34/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1036 - loss: -8196856832.0000 - val_accuracy: 0.1017 - val_loss: -9077718016.0000\n",
            "Epoch 35/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1044 - loss: -8926385152.0000 - val_accuracy: 0.1039 - val_loss: -9628206080.0000\n",
            "Epoch 36/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.1035 - loss: -9681723392.0000 - val_accuracy: 0.1053 - val_loss: -10718654464.0000\n",
            "Epoch 37/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1042 - loss: -10470726656.0000 - val_accuracy: 0.1059 - val_loss: -10730321920.0000\n",
            "Epoch 38/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1020 - loss: -11288562688.0000 - val_accuracy: 0.1056 - val_loss: -11935034368.0000\n",
            "Epoch 39/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.1002 - loss: -12201250816.0000 - val_accuracy: 0.1107 - val_loss: -13019386880.0000\n",
            "Epoch 40/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1038 - loss: -13176614912.0000 - val_accuracy: 0.1052 - val_loss: -13717085184.0000\n",
            "Epoch 41/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1021 - loss: -14056563712.0000 - val_accuracy: 0.1032 - val_loss: -14714798080.0000\n",
            "Epoch 42/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1014 - loss: -15129377792.0000 - val_accuracy: 0.1008 - val_loss: -16476254208.0000\n",
            "Epoch 43/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1029 - loss: -16064366592.0000 - val_accuracy: 0.1030 - val_loss: -17476370432.0000\n",
            "Epoch 44/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1020 - loss: -17293549568.0000 - val_accuracy: 0.1088 - val_loss: -17959577600.0000\n",
            "Epoch 45/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.1026 - loss: -18437355520.0000 - val_accuracy: 0.1049 - val_loss: -19802890240.0000\n",
            "Epoch 46/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.1046 - loss: -19586633728.0000 - val_accuracy: 0.1079 - val_loss: -21272178688.0000\n",
            "Epoch 47/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1031 - loss: -20845703168.0000 - val_accuracy: 0.1032 - val_loss: -22428207104.0000\n",
            "Epoch 48/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1040 - loss: -22155177984.0000 - val_accuracy: 0.1016 - val_loss: -23608489984.0000\n",
            "Epoch 49/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.1061 - loss: -23565060096.0000 - val_accuracy: 0.1014 - val_loss: -26084352000.0000\n",
            "Epoch 50/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.1004 - loss: -25137709056.0000 - val_accuracy: 0.1040 - val_loss: -26786779136.0000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1034 - loss: -26237519872.0000\n",
            "Loss: -26786779136.0000, Accuracy: 0.1040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Charger les données Fashion MNIST\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalisation\n",
        "\n",
        "# Créer un modèle séquentiel\n",
        "model = Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    Dropout(0.5),  # Dropout avec un taux de 0.5 (désactive 50% des neurones)\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compiler le modèle\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entraîner le modèle\n",
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# Évaluer le modèle\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0XhWVLLGuJU",
        "outputId": "64ac00bd-7f7c-42bc-ab48-8d42bc6635e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7019 - loss: 0.8203 - val_accuracy: 0.8393 - val_loss: 0.4516\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8206 - loss: 0.4958 - val_accuracy: 0.8378 - val_loss: 0.4356\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8359 - loss: 0.4496 - val_accuracy: 0.8527 - val_loss: 0.4032\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8412 - loss: 0.4334 - val_accuracy: 0.8627 - val_loss: 0.3873\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8503 - loss: 0.4091 - val_accuracy: 0.8585 - val_loss: 0.3880\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8544 - loss: 0.4009 - val_accuracy: 0.8635 - val_loss: 0.3823\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8562 - loss: 0.3908 - val_accuracy: 0.8681 - val_loss: 0.3598\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8589 - loss: 0.3810 - val_accuracy: 0.8657 - val_loss: 0.3692\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8612 - loss: 0.3766 - val_accuracy: 0.8698 - val_loss: 0.3654\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8635 - loss: 0.3678 - val_accuracy: 0.8708 - val_loss: 0.3612\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8751 - loss: 0.3610\n",
            "Loss: 0.3612, Accuracy: 0.8708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP"
      ],
      "metadata": {
        "id": "aewnL0ym4Zq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compléter le code pour entraîner un réseau de neurones simple sur la base MNIST. Comparer les trois variantes de descente de gradient :\n",
        "\n",
        "*   SGD (stochastique)\n",
        "*   Batch Gradient Descent\n",
        "*   Mini-Batch Gradient Descent\n",
        "\n",
        "Tester différentes techniques d’optimisation couramment utilisées en Deep Learning :\n",
        "\n",
        "*   Batch Normalization\n",
        "*   Dropout\n",
        "*   Early Stopping\n",
        "*   Grid Search pour trouver les meilleurs hyperparamètres.\n"
      ],
      "metadata": {
        "id": "4ckMmHJi4hk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# (Optionnel mais utile en mode démonstration)\n",
        "# tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# Charger les données MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0  # Normalisation\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        keras.Input(shape=(28, 28)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Stochastic Gradient Descent (batch_size=1)\n",
        "print(\"--- SGD ---\")\n",
        "model_sgd = create_model()  # <--- On met le résultat dans model_sgd\n",
        "model_sgd.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_sgd.fit(x_train, y_train, epochs=3, batch_size=1)\n",
        "# Batch Gradient Descent (full dataset)\n",
        "print(\"\\n--- Batch (Full) ---\")\n",
        "model_batch = create_model() # <--- On met le résultat dans model_batch\n",
        "model_batch.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_batch.fit(x_train, y_train, epochs=3, batch_size=len(x_train))\n",
        "# Mini-Batch Gradient Descent (batch_size=32)\n",
        "print(\"\\n--- Mini-Batch ---\")\n",
        "model_mini = create_model() # <--- On met le résultat dans model_mini\n",
        "model_mini.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_mini.fit(x_train, y_train, epochs=3, batch_size=32)"
      ],
      "metadata": {
        "id": "zzj_OGDO6wRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# ==========================================\n",
        "# 1. PRÉPARATION DES DONNÉES\n",
        "# ==========================================\n",
        "print(\"Chargement des données...\")\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalisation (0-255 -> 0-1)\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Fonction de base pour créer un modèle simple (réutilisée plusieurs fois)\n",
        "def create_simple_model():\n",
        "    model = Sequential([\n",
        "        keras.Input(shape=(28, 28)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# 2. PARTIE A : COMPARAISON DES VARIANTES DE GRADIENT\n",
        "# ==========================================\n",
        "print(\"\\n=== PARTIE A : COMPARAISON SGD vs BATCH vs MINI-BATCH ===\")\n",
        "\n",
        "# --- A1. SGD (Stochastique) ---\n",
        "# Attention : batch_size=1 est très lent. On met peu d'epochs pour l'exemple.\n",
        "print(\"\\n1. Entraînement SGD (batch_size=1)...\")\n",
        "model_sgd = create_simple_model()\n",
        "model_sgd.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "# Note : Pour le TP, tu peux réduire x_train[:1000] si c'est trop long sur ton PC\n",
        "model_sgd.fit(x_train, y_train, epochs=2, batch_size=1, verbose=1)\n",
        "\n",
        "# --- A2. Batch Gradient Descent (Tout le dataset) ---\n",
        "print(\"\\n2. Entraînement Batch (batch_size=Tout)...\")\n",
        "model_batch = create_simple_model()\n",
        "model_batch.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_batch.fit(x_train, y_train, epochs=5, batch_size=len(x_train), verbose=1)\n",
        "\n",
        "# --- A3. Mini-Batch Gradient Descent (Standard) ---\n",
        "print(\"\\n3. Entraînement Mini-Batch (batch_size=32)...\")\n",
        "model_mini = create_simple_model()\n",
        "model_mini.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model_mini.fit(x_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 3. PARTIE B : TECHNIQUES D'OPTIMISATION\n",
        "# (BatchNormalization, Dropout, Early Stopping)\n",
        "# ==========================================\n",
        "print(\"\\n=== PARTIE B : MODÈLE OPTIMISÉ ===\")\n",
        "\n",
        "def create_optimized_model():\n",
        "    model = Sequential([\n",
        "        keras.Input(shape=(28, 28)),\n",
        "        Flatten(),\n",
        "\n",
        "        # Couche cachée avec BatchNorm et Dropout\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(), # Stabilise l'apprentissage\n",
        "        Dropout(0.2),         # Éteint 20% des neurones (Anti-Surapprentissage)\n",
        "\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model_opt = create_optimized_model()\n",
        "model_opt.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early Stopping : Arrête si 'val_loss' ne s'améliore pas pendant 3 époques\n",
        "arret_precoce = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "print(\"Lancement de l'entraînement optimisé avec Early Stopping...\")\n",
        "history = model_opt.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,                  # On prévoit large, l'EarlyStopping arrêtera avant\n",
        "    batch_size=32,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[arret_precoce],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ==========================================\n",
        "# 4. PARTIE C : GRID SEARCH MANUEL (Hyperparamètres)\n",
        "# ==========================================\n",
        "print(\"\\n=== PARTIE C : GRID SEARCH (Recherche du meilleur modèle) ===\")\n",
        "\n",
        "# Paramètres à tester\n",
        "liste_neurones = [64, 128]\n",
        "liste_batch_sizes = [32, 64]\n",
        "meilleur_score = 0\n",
        "meilleurs_params = {}\n",
        "\n",
        "for neurons in liste_neurones:\n",
        "    for batch in liste_batch_sizes:\n",
        "        print(f\"Test avec {neurons} neurones et batch_size {batch}...\")\n",
        "\n",
        "        # Création du modèle personnalisé\n",
        "        model_grid = Sequential([\n",
        "            keras.Input(shape=(28, 28)),\n",
        "            Flatten(),\n",
        "            Dense(neurons, activation='relu'), # On utilise la variable de la boucle\n",
        "            Dense(10, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        model_grid.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Entraînement court (3 epochs juste pour comparer)\n",
        "        model_grid.fit(x_train, y_train, epochs=3, batch_size=batch, verbose=0)\n",
        "\n",
        "        # Évaluation sur le test\n",
        "        _, accuracy = model_grid.evaluate(x_test, y_test, verbose=0)\n",
        "        print(f\" -> Précision obtenue : {accuracy:.4f}\")\n",
        "\n",
        "        # On garde le champion\n",
        "        if accuracy > meilleur_score:\n",
        "            meilleur_score = accuracy\n",
        "            meilleurs_params = {'neurones': neurons, 'batch_size': batch}\n",
        "\n",
        "print(\"\\n---------------------------------------\")\n",
        "print(f\"RÉSULTAT GRID SEARCH :\")\n",
        "print(f\"Meilleurs paramètres trouvés : {meilleurs_params}\")\n",
        "print(f\"Meilleure précision : {meilleur_score:.4f}\")\n",
        "print(\"---------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x3zEF0EmEU4",
        "outputId": "25c03d4e-63a2-4865-f890-887e46649fec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chargement des données...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "=== PARTIE A : COMPARAISON SGD vs BATCH vs MINI-BATCH ===\n",
            "\n",
            "1. Entraînement SGD (batch_size=1)...\n",
            "Epoch 1/2\n",
            "\u001b[1m60000/60000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2ms/step - accuracy: 0.8964 - loss: 0.3425\n",
            "Epoch 2/2\n",
            "\u001b[1m60000/60000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2ms/step - accuracy: 0.9678 - loss: 0.1035\n",
            "\n",
            "2. Entraînement Batch (batch_size=Tout)...\n",
            "Epoch 1/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.1189 - loss: 2.3828\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 673ms/step - accuracy: 0.1236 - loss: 2.3698\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 677ms/step - accuracy: 0.1286 - loss: 2.3572\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 675ms/step - accuracy: 0.1347 - loss: 2.3449\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 669ms/step - accuracy: 0.1408 - loss: 2.3329\n",
            "\n",
            "3. Entraînement Mini-Batch (batch_size=32)...\n",
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7276 - loss: 1.0539\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9018 - loss: 0.3591\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9137 - loss: 0.3050\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9241 - loss: 0.2724\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9332 - loss: 0.2412\n",
            "\n",
            "=== PARTIE B : MODÈLE OPTIMISÉ ===\n",
            "Lancement de l'entraînement optimisé avec Early Stopping...\n",
            "Epoch 1/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8707 - loss: 0.4288 - val_accuracy: 0.9600 - val_loss: 0.1274\n",
            "Epoch 2/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9505 - loss: 0.1630 - val_accuracy: 0.9687 - val_loss: 0.1032\n",
            "Epoch 3/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9614 - loss: 0.1251 - val_accuracy: 0.9755 - val_loss: 0.0813\n",
            "Epoch 4/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9660 - loss: 0.1104 - val_accuracy: 0.9759 - val_loss: 0.0823\n",
            "Epoch 5/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9700 - loss: 0.0949 - val_accuracy: 0.9750 - val_loss: 0.0812\n",
            "Epoch 6/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9708 - loss: 0.0892 - val_accuracy: 0.9755 - val_loss: 0.0810\n",
            "Epoch 7/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9743 - loss: 0.0817 - val_accuracy: 0.9774 - val_loss: 0.0746\n",
            "Epoch 8/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9760 - loss: 0.0744 - val_accuracy: 0.9768 - val_loss: 0.0739\n",
            "Epoch 9/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9760 - loss: 0.0723 - val_accuracy: 0.9787 - val_loss: 0.0736\n",
            "Epoch 10/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9775 - loss: 0.0697 - val_accuracy: 0.9791 - val_loss: 0.0721\n",
            "Epoch 11/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9797 - loss: 0.0631 - val_accuracy: 0.9799 - val_loss: 0.0677\n",
            "Epoch 12/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9812 - loss: 0.0563 - val_accuracy: 0.9793 - val_loss: 0.0723\n",
            "Epoch 13/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9803 - loss: 0.0597 - val_accuracy: 0.9796 - val_loss: 0.0699\n",
            "Epoch 14/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9816 - loss: 0.0549 - val_accuracy: 0.9810 - val_loss: 0.0661\n",
            "Epoch 15/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9814 - loss: 0.0573 - val_accuracy: 0.9804 - val_loss: 0.0657\n",
            "Epoch 16/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9826 - loss: 0.0516 - val_accuracy: 0.9801 - val_loss: 0.0668\n",
            "Epoch 17/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9829 - loss: 0.0526 - val_accuracy: 0.9808 - val_loss: 0.0676\n",
            "Epoch 18/50\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9853 - loss: 0.0436 - val_accuracy: 0.9797 - val_loss: 0.0697\n",
            "\n",
            "=== PARTIE C : GRID SEARCH (Recherche du meilleur modèle) ===\n",
            "Test avec 64 neurones et batch_size 32...\n",
            " -> Précision obtenue : 0.9686\n",
            "Test avec 64 neurones et batch_size 64...\n",
            " -> Précision obtenue : 0.9624\n",
            "Test avec 128 neurones et batch_size 32...\n",
            " -> Précision obtenue : 0.9740\n",
            "Test avec 128 neurones et batch_size 64...\n",
            " -> Précision obtenue : 0.9719\n",
            "\n",
            "---------------------------------------\n",
            "RÉSULTAT GRID SEARCH :\n",
            "Meilleurs paramètres trouvés : {'neurones': 128, 'batch_size': 32}\n",
            "Meilleure précision : 0.9740\n",
            "---------------------------------------\n"
          ]
        }
      ]
    }
  ]
}